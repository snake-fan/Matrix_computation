\documentclass{article}

\title{\Huge Matrix Computation Homework 1}
\author{\normalsize Yifan Zhang  2025251018  zhangyf52025@shanghaitech.edu.cn}

% import peckage
\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{array}
\usepackage{mathtools}
\usepackage{parskip}

\newcommand{\madj}{\mathop{\mathrm{adj}}}
\newcommand{\trace}{\mathop{\mathrm{tr}}}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\pnorm}{\lVert}{\rVert_p}

% special letters
\newcommand{\x}{{\mathbf{x}}} % vector x
\newcommand{\y}{{\mathbf{y}}} % vector y
\newcommand{\z}{{\mathbf{z}}} % vector z
\newcommand{\0}{{\mathbf{0}}} % zero vector

\newcommand{\A}{{\mathbf{A}}} % matrix A
\newcommand{\B}{{\mathbf{B}}} % matrix B
\newcommand{\matC}{{\mathbf{C}}} % matrix C
\newcommand{\matD}{{\mathbf{D}}} % matrix D
\newcommand{\matE}{{\mathbf{E}}} % matrix E
\newcommand{\matH}{{\mathbf{H}}} % matrix H
\newcommand{\matK}{{\mathbf{K}}} % matrix K
\newcommand{\X}{{\mathbf{X}}} % matrix X
\newcommand{\Y}{{\mathbf{Y}}} % matrix Y
\newcommand{\Z}{{\mathbf{Z}}} % matrix Z
\newcommand{\M}{{\mathbf{M}}} % matrix M
\newcommand{\I}{{\mathbf{I}}} % identity matrix

\newcommand{\calV}{{\mathcal{V}}} % subspace V
\newcommand{\calU}{{\mathcal{U}}} % subspace U
\newcommand{\calS}{{\mathcal{S}}} % subspace S
\newcommand{\calM}{{\mathcal{M}}} % subspace M
\newcommand{\calN}{{\mathcal{N}}} % subspace N
\newcommand{\calR}{\mathop{\mathcal{R}}} % range
\renewcommand{\calN}{\mathop{\mathcal{N}}} % nullspace

% number fields
\newcommand{\R}{\mathbb{R}} % real numbers
\newcommand{\C}{\mathbb{C}} % complex numbers

\newcommand{\bigO}{{\mathcal{O}}} % big-Oh notation

\newcommand{\vect}[1]{\mathbf{#1}}

\begin{document}
\maketitle

\section*{Problem 1. (Discrete-Time LTI Systems)}

\subsection*{1)}

\textbf{Solution.}

According to the problem description, $\mathbf{A}$ is a convolution matrix constructed from $h[n] = [h_0, h_1, h_2]$. $\mathbf{y} = \mathbf{A}\mathbf{x}$ describes the linear convolution $y[n] = x[n] * h[n]$.

\[
y[n] = \sum_{k=0}^{p} h[k] \cdot x[n-k] = h_0 x[n] + h_1 x[n-1] + h_2 x[n-2]
\]

Since there is no noise, we can directly solve for $h_0, h_1, h_2$ using the first few outputs.

\begin{itemize}
\item \textbf{Solving for $h_0$}:
  \[
    y[0] = h_0 x[0] + h_1 x[-1] + h_2 x[-2]
  \]
  Since $x[n<0] = 0$, we have:
  \[
    y[0] = h_0 x[0]
  \]
  \[
    1 = h_0 \cdot 1 \implies \mathbf{h_0 = 1}
  \]

\item \textbf{Solving for $h_1$}:
  \[
    y[1] = h_0 x[1] + h_1 x[0] + h_2 x[-1]
  \]
  \[
    y[1] = h_0 x[1] + h_1 x[0]
  \]
  \[
    2 = (1) \cdot (0) + h_1 \cdot (1) \implies \mathbf{h_1 = 2}
  \]

\item \textbf{Solving for $h_2$}:
  \[
    y[2] = h_0 x[2] + h_1 x[1] + h_2 x[0]
  \]
  \[
    4 = (1) \cdot (1) + (2) \cdot (0) + h_2 \cdot (1)
  \]
  \[
    4 = 1 + h_2 \implies \mathbf{h_2 = 3}
  \]
\end{itemize}

We have obtained the channel impulse response $\vect{h} = [1, 2, 3]^T$.

Since
\[
\mathbf{A} =
\begin{pmatrix}
  h_0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  h_1 & h_0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  h_2 & h_1 & h_0 & 0 & 0 & 0 & 0 & 0 \\
  0 & h_2 & h_1 & h_0 & 0 & 0 & 0 & 0 \\
  0 & 0 & h_2 & h_1 & h_0 & 0 & 0 & 0 \\
  0 & 0 & 0 & h_2 & h_1 & h_0 & 0 & 0 \\
  0 & 0 & 0 & 0 & h_2 & h_1 & h_0 & 0 \\
  0 & 0 & 0 & 0 & 0 & h_2 & h_1 & h_0
\end{pmatrix}
\]
Substituting $h_0=1, h_1=2, h_2=3$, we solve for $\mathbf{A}$ as:
\[
\mathbf{A} =
\begin{pmatrix}
  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  2 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  3 & 2 & 1 & 0 & 0 & 0 & 0 & 0 \\
  0 & 3 & 2 & 1 & 0 & 0 & 0 & 0 \\
  0 & 0 & 3 & 2 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 3 & 2 & 1 & 0 & 0 \\
  0 & 0 & 0 & 0 & 3 & 2 & 1 & 0 \\
  0 & 0 & 0 & 0 & 0 & 3 & 2 & 1
\end{pmatrix}
\]
The full equation for $\vect{y} = \mathbf{A}\vect{x}$ is:
\[
\begin{pmatrix} 1 \\ 2 \\ 4 \\ 2 \\ 4 \\ 3 \\ 5 \\ 4
\end{pmatrix}
=
\begin{pmatrix}
  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
  2 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  3 & 2 & 1 & 0 & 0 & 0 & 0 & 0 \\
  0 & 3 & 2 & 1 & 0 & 0 & 0 & 0 \\
  0 & 0 & 3 & 2 & 1 & 0 & 0 & 0 \\
  0 & 0 & 0 & 3 & 2 & 1 & 0 & 0 \\
  0 & 0 & 0 & 0 & 3 & 2 & 1 & 0 \\
  0 & 0 & 0 & 0 & 0 & 3 & 2 & 1
\end{pmatrix}
\begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 1 \\ 0 \\ 1
\end{pmatrix}
\]

\subsection*{b)}

\textbf{Solution.}

Here, $\vect{y} = \mathbf{C}\hat{\vect{h}}$, where $\mathbf{C}$ is the convolution matrix constructed from the input $\vect{x}$, and $\hat{\vect{h}}$ is the channel response vector we want to estimate.

\textbf{Constructing the matrix $\mathbf{C}$}

It is obviously that:
\[
\mathbf{C} =
\begin{pmatrix}
x[0] & x[-1] & x[-2] \\
x[1] & x[0] & x[-1] \\
x[2] & x[1] & x[0] \\
x[3] & x[2] & x[1] \\
x[4] & x[3] & x[2] \\
x[5] & x[4] & x[3] \\
x[6] & x[5] & x[4] \\
x[7] & x[6] & x[5]
\end{pmatrix}
\]
Substituting $\vect{x} = [ 1, 0, 1, 0, 1, 1, 0, 1]$ and $x[n<0]=0$:
\[
\mathbf{C} =
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1
\end{pmatrix}
\]

\textbf{Setting up the Normal Equations}

The least-squares solution $\hat{\vect{h}}$ is given by the normal equations:
\[
(\mathbf{C}^T \mathbf{C}) \hat{\vect{h}} = \mathbf{C}^T \vect{y}
\]

\textbf{Calculating $\mathbf{C}^T \mathbf{C}$}
\[
\mathbf{C}^T \mathbf{C} =
\begin{pmatrix}
1 & 0 & 1 & 0 & 1 & 1 & 0 & 1 \\
0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 \\
0 & 0 & 1 & 0 & 1 & 0 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1
\end{pmatrix}
=
\begin{pmatrix}
5 & 1 & 3 \\
1 & 4 & 1 \\
3 & 1 & 4
\end{pmatrix}
\]

\textbf{Calculating $\mathbf{C}^T \vect{y}$}
\[
\mathbf{C}^T \vect{y} =
\begin{pmatrix}
1 & 0 & 1 & 0 & 1 & 1 & 0 & 1 \\
0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 \\
0 & 0 & 1 & 0 & 1 & 0 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
1 \\ 2 \\ 4 \\ 2 \\ 4 \\ 3 \\ 5 \\ 4
\end{pmatrix}
=
\begin{pmatrix}
16 \\
12 \\
17
\end{pmatrix}
\]

\textbf{Solving for $\hat{\vect{h}}$} \\
We now need to solve the linear system:
\[
\begin{pmatrix}
5 & 1 & 3 \\
1 & 4 & 1 \\
3 & 1 & 4
\end{pmatrix}
\begin{pmatrix}
h_0 \\ h_1 \\ h_2
\end{pmatrix}
=
\begin{pmatrix}
16 \\
12 \\
17
\end{pmatrix}
\]
\[
\hat{\vect{h}} =
\begin{pmatrix} h_0 \\ h_1 \\ h_2
\end{pmatrix} =
\begin{pmatrix} 1 \\ 2 \\ 3
\end{pmatrix}
\]

\section*{Problem 2. (Gram-Schmidt Procedure) }
\subsection*{1)}

\textbf{Solution.}

\textbf{Step 1: Compute $\vect{q}_1$}
\[
r_{11} = \|\mathbf{a}_1\|_2 = \sqrt{1^2 + 1^2 + 1^2 + 1^2} = 2
\]
\[
\mathbf{q}_1 = \frac{\mathbf{a}_1}{r_{11}} =
\begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \\ 0.5
\end{bmatrix}
\]

\textbf{Step 2: Compute $\mathbf{q}_2$}
\[
r_{12} = \mathbf{q}_1^T \mathbf{a}_2 = 0.5(0) + 0.5(2) + 0.5(1) + 0.5(1) = 2
\]
\[
\mathbf{v}_2 = \mathbf{a}_2 - r_{12}\mathbf{q}_1 =
\begin{bmatrix} 0 \\ 2 \\ 1 \\ 1
\end{bmatrix} - 2
\begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \\ 0.5
\end{bmatrix} =
\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0
\end{bmatrix}
\]
\[
r_{22} = \|\mathbf{v}_2\|_2 = \sqrt{2}
\]
\[
\mathbf{q}_2 = \frac{\mathbf{v}_2}{r_{22}} =
\begin{bmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \\ 0
\end{bmatrix}
\]

\textbf{Step 3: Compute $\mathbf{q}_3$}
\[
r_{13} = \mathbf{q}_1^T \mathbf{a}_3 = 3
\]
\[
r_{23} = \mathbf{q}_2^T \mathbf{a}_3 = \frac{1}{\sqrt{2}}(-3 + 1) = -\sqrt{2}
\]
\[
\mathbf{v}_3 = \mathbf{a}_3 - r_{13}\mathbf{q}_1 - r_{23}\mathbf{q}_2
=
\begin{bmatrix} 3 \\ 1 \\ 1 \\ 1
\end{bmatrix} -
\begin{bmatrix} 1.5 \\ 1.5 \\ 1.5 \\ 1.5
\end{bmatrix} -
\begin{bmatrix} 1 \\ -1 \\ 0 \\ 0
\end{bmatrix}
=
\begin{bmatrix} 0.5 \\ 0.5 \\ -0.5 \\ -0.5
\end{bmatrix}
\]
\[
r_{33} = \|\mathbf{v}_3\|_2 = 1
\]
\[
\mathbf{q}_3 =
\begin{bmatrix} 0.5 \\ 0.5 \\ -0.5 \\ -0.5
\end{bmatrix}
\]

\textbf{Result:}
\[
\mathbf{Q} =
\begin{bmatrix} 0.5 & -1/\sqrt{2} & 0.5 \\ 0.5 & 1/\sqrt{2} & 0.5 \\ 0.5 & 0 & -0.5 \\ 0.5 & 0 & -0.5
\end{bmatrix}, \quad
\mathbf{R} =
\begin{bmatrix} 2 & 2 & 3 \\ 0 & \sqrt{2} & -\sqrt{2} \\ 0 & 0 & 1
\end{bmatrix}
\]

\newpage

\subsection*{2) QR Decomposition of $\mathbf{A}_1$ (Modified Gram-Schmidt)}

\textbf{Initialization}
Let $\mathbf{v}_1 = \mathbf{a}_1, \mathbf{v}_2 = \mathbf{a}_2, \mathbf{v}_3 = \mathbf{a}_3$.

\textbf{Step 1}
\[ r_{11} = \|\mathbf{v}_1\| = 2, \quad \mathbf{q}_1 = \mathbf{v}_1 / 2 = [0.5, 0.5, 0.5, 0.5]^T \]
Update $\mathbf{v}_2, \mathbf{v}_3$:
\[ r_{12} = \mathbf{q}_1^T \mathbf{v}_2 = 2 \implies \mathbf{v}_2^{(2)} = \mathbf{v}_2 - r_{12}\mathbf{q}_1 = [-1, 1, 0, 0]^T \]
\[ r_{13} = \mathbf{q}_1^T \mathbf{v}_3 = 3 \implies \mathbf{v}_3^{(2)} = \mathbf{v}_3 - r_{13}\mathbf{q}_1 = [1.5, -0.5, -0.5, -0.5]^T \]

\textbf{Step 2}
\[ r_{22} = \|\mathbf{v}_2^{(2)}\| = \sqrt{2}, \quad \mathbf{q}_2 = \mathbf{v}_2^{(2)} / \sqrt{2} = [\frac{-1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0, 0]^T \]
Update $\mathbf{v}_3$:
\[ r_{23} = \mathbf{q}_2^T \mathbf{v}_3^{(2)} = -\sqrt{2} \]
\[ \mathbf{v}_3^{(3)} = \mathbf{v}_3^{(2)} - r_{23}\mathbf{q}_2 = [0.5, 0.5, -0.5, -0.5]^T \]

\textbf{Step 3}
\[ r_{33} = \|\mathbf{v}_3^{(3)}\| = 1, \quad \mathbf{q}_3 = [0.5, 0.5, -0.5, -0.5]^T \]

\textbf{Result:}

\[
\mathbf{Q} =
\begin{bmatrix} 0.5 & -1/\sqrt{2} & 0.5 \\ 0.5 & 1/\sqrt{2} & 0.5 \\ 0.5 & 0 & -0.5 \\ 0.5 & 0 & -0.5
\end{bmatrix}, \quad
\mathbf{R} =
\begin{bmatrix} 2 & 2 & 3 \\ 0 & \sqrt{2} & -\sqrt{2} \\ 0 & 0 & 1
\end{bmatrix}
\]

\subsection*{3) QR Decomposition of $\mathbf{A}_2$}

Given $\mathbf{A}_2 = [\mathbf{a}_1, \mathbf{a}_2, \mathbf{a}_3, \mathbf{a}_4]$ with rank 3. We compute $\mathbf{Q} \in \mathbb{R}^{6 \times 3}$ and $\mathbf{R} \in \mathbb{R}^{3 \times 4}$.

\textbf{Processing Columns}

\textbf{Vector $\mathbf{a}_1$:}

\[\|\mathbf{a}_1\| = 2\]
\[ \mathbf{q}_1 = [0.5, 0.5, 0, 0, 0.5, 0.5]^T, \quad r_{11}=2 \]

\textbf{Vector $\mathbf{a}_2$:}

\[r_{12} = \mathbf{q}_1^T \mathbf{a}_2 = 1\]
\[ \mathbf{v}_2 = \mathbf{a}_2 - r_{12} \mathbf{q}_1 = [-0.5, 0.5, 0, 1, 0.5, -0.5]^T, \quad \|\mathbf{v}_2\|=\sqrt{2} \]
\[ \mathbf{q}_2 = \frac{1}{\sqrt{2}}[-0.5, 0.5, 0, 1, 0.5, -0.5]^T, \quad r_{22}=\sqrt{2} \]

\textbf{Vector $\mathbf{a}_3$:}
\[ r_{13} = \mathbf{q}_1^T \mathbf{a}_3 = 7, \quad r_{23} = \mathbf{q}_2^T \mathbf{a}_3 = \sqrt{2} \]
Since $\mathbf{v}_3 = \mathbf{a}_3 - 7\mathbf{q}_1 - \sqrt{2}\mathbf{q}_2 = \mathbf{0}$. Thus, no new basis vector is formed.

\[ r_{14} = \mathbf{q}_1^T \mathbf{a}_4 = 1.5 \]
\[ r_{24} = \mathbf{q}_2^T \mathbf{a}_4 = \frac{1}{\sqrt{2}}(0.5) = \frac{1}{2\sqrt{2}} \]
\[ \mathbf{v}_4 = \mathbf{a}_4 - 1.5\mathbf{q}_1 - \frac{1}{2\sqrt{2}}\mathbf{q}_2 = \frac{1}{8}[3, 1, 8, -2, 1, -5]^T \]
\[ r_{34} = \|\mathbf{v}_4\| = \frac{\sqrt{26}}{4} \]
\[ \mathbf{q}_3 = \frac{\mathbf{v}_4}{r_{34}} = \frac{1}{2\sqrt{26}}[3, 1, 8, -2, 1, -5]^T \]

\textbf{Result:}
\[
\mathbf{Q} =
\begin{bmatrix} 0.5 & \frac{-1}{2\sqrt{2}} & \frac{3}{2\sqrt{26}} \\ 0.5 & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{26}} \\ 0 & 0 & \frac{8}{2\sqrt{26}} \\ 0 & \frac{1}{\sqrt{2}} & \frac{-2}{2\sqrt{26}} \\ 0.5 & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{26}} \\ 0.5 & \frac{-1}{2\sqrt{2}} & \frac{-5}{2\sqrt{26}}
\end{bmatrix}, \quad
\mathbf{R} =
\begin{bmatrix} 2 & 1 & 7 & 1.5 \\ 0 & \sqrt{2} & \sqrt{2} & \frac{\sqrt{2}}{4} \\ 0 & 0 & 0 & \frac{\sqrt{26}}{4}
\end{bmatrix}
\]

\subsection*{4)}

Since $A = QR, \quad Ax = b$,
we can easily get $\mathbf{R}\mathbf{x} = \mathbf{Q}^T\mathbf{b}$:

\[
\mathbf{Q}^T\mathbf{b} =
\begin{bmatrix} \mathbf{q}_1^T\mathbf{b} \\ \mathbf{q}_2^T\mathbf{b} \\ \mathbf{q}_3^T\mathbf{b}
\end{bmatrix} =
\begin{bmatrix} 0.5(4)+0+0.5(4)+0 \\ \frac{-1}{\sqrt{2}}(4)+0+0+0 \\ 0.5(4)+0-0.5(4)+0
\end{bmatrix} =
\begin{bmatrix} 4 \\ -2\sqrt{2} \\ 0
\end{bmatrix}
\]

Solving the system:
\[
\begin{bmatrix} 2 & 2 & 3 \\ 0 & \sqrt{2} & -\sqrt{2} \\ 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ x_3
\end{bmatrix} =
\begin{bmatrix} 4 \\ -2\sqrt{2} \\ 0
\end{bmatrix}
\]

\begin{itemize}
\item $1 \cdot x_3 = 0 \implies x_3 = 0$.
\item $\sqrt{2}x_2 - \sqrt{2}(0) = -2\sqrt{2} \implies x_2 = -2$.
\item $2x_1 + 2(-2) + 3(0) = 4 \implies 2x_1 - 4 = 4 \implies 2x_1 = 8 \implies x_1 = 4$.
\end{itemize}

\textbf{Solution:} $\mathbf{x} = [4, -2, 0]^T$.

\section*{Problem 3. (Householder Reï¬‚ection)}

\subsection*{1)}

\textbf{Step 1: First Householder Reflection ($H_1$)}

We focus on the first column of $A$, $\vect{x} = A_1 =
\begin{bmatrix} 1 \\ -2 \\ 2
\end{bmatrix}$.

First, find its norm:
$$ \|\vect{x}\|_2 = \sqrt{1^2 + (-2)^2 + 2^2} = 3 $$

Next, we define the vector $\vect{v}_1$. The problem states to choose the sign $\mp$ to maximize $\|\vect{v}\|_2$.
\begin{itemize}
\item $\vect{v}_- = \vect{x} - \|\vect{x}\|_2\vect{e}_1 =
\begin{bmatrix} 1 \\ -2 \\ 2
\end{bmatrix} - 3
\begin{bmatrix} 1 \\ 0 \\ 0
\end{bmatrix} =
\begin{bmatrix} -2 \\ -2 \\ 2
\end{bmatrix}$. $\|\vect{v}_-\|_2^2 = 4+4+4 = 12$.
\item $\vect{v}_+ = \vect{x} + \|\vect{x}\|_2\vect{e}_1 =
\begin{bmatrix} 1 \\ -2 \\ 2
\end{bmatrix} + 3
\begin{bmatrix} 1 \\ 0 \\ 0
\end{bmatrix} =
\begin{bmatrix} 4 \\ -2 \\ 2
\end{bmatrix}$. $\|\vect{v}_+\|_2^2 = 16+4+4 = 24$.
\end{itemize}
So, $\vect{v}_1 =
\begin{bmatrix} 4 \\ -2 \\ 2
\end{bmatrix}$, and $\|\vect{v}_1\|_2^2 = 24$.

The Householder reflector is $H_1 = I - 2 \frac{\vect{v}_1 \vect{v}_1^T}{\|\vect{v}_1\|_2^2}$:
$$ \vect{v}_1 \vect{v}_1^T =
\begin{bmatrix} 4 \\ -2 \\ 2
\end{bmatrix}
\begin{bmatrix} 4 & -2 & 2
\end{bmatrix} =
\begin{bmatrix} 16 & -8 & 8 \\ -8 & 4 & -4 \\ 8 & -4 & 4
\end{bmatrix} $$
$$ H_1 = I - \frac{2}{24}
\begin{bmatrix} 16 & -8 & 8 \\ -8 & 4 & -4 \\ 8 & -4 & 4
\end{bmatrix} =\frac{1}{3}
\begin{bmatrix} -1 & 2 & -2 \\ 2 & 2 & 1 \\ -2 & 1 & 2
\end{bmatrix} $$

\textbf{Step 2: Apply $H_1$ to $A$}

We compute $A^{(1)} = H_1 A$:
$$ A^{(1)} = \frac{1}{3}
\begin{bmatrix} -1 & 2 & -2 \\ 2 & 2 & 1 \\ -2 & 1 & 2
\end{bmatrix}
\begin{bmatrix} 1 & 1 & -3 \\ -2 & -5 & 20 \\ 2 & 8 & 3
\end{bmatrix} =
\begin{bmatrix} -3 & -9 & 37/3 \\ 0 & 0 & 37/3 \\ 0 & 3 & 32/3
\end{bmatrix} $$
As expected, the first column below the diagonal is zero.

\textbf{Step 3: Second Householder Reflection ($H_2$)}

We now work on the $2 \times 2$ submatrix $
\begin{bmatrix} 0 & 37/3 \\ 3 & 32/3
\end{bmatrix}$.

We want to zero out the $(2,1)$ entry of this submatrix.

Let $\vect{x}' =
\begin{bmatrix} 0 \\ 3
\end{bmatrix}$. $\|\vect{x}'\|_2 = 3$.
We choose $\vect{v}' = \vect{x}' + \|\vect{x}'\|_2 \vect{e}_1 =
\begin{bmatrix} 0 \\ 3
\end{bmatrix} + 3
\begin{bmatrix} 1 \\ 0
\end{bmatrix} =
\begin{bmatrix} 3 \\ 3
\end{bmatrix}$.
$\|\vect{v}'\|_2^2 = 9 + 9 = 18$.

The $2 \times 2$ reflector $H'$ is:
$$
H' = I - 2 \frac{\vect{v}' \vect{v}'^T}{\|\vect{v}'\|_2^2} = I - \frac{2}{18}
\begin{bmatrix} 3 \\ 3
\end{bmatrix}
\begin{bmatrix} 3 & 3
\end{bmatrix} =
\begin{bmatrix} 0 & -1 \\ -1 & 0
\end{bmatrix}
$$

This is a simple permutation matrix. We embed this into a $3 \times 3$ matrix $H_2$:
$$ H_2 =
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & -1 & 0
\end{bmatrix} $$

\textbf{Step 4: Find $R$}

The final upper triangular matrix $R$ is $R = H_2 A^{(1)}$:
$$ R =
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & -1 & 0
\end{bmatrix}
\begin{bmatrix} -3 & -9 & 37/3 \\ 0 & 0 & 37/3 \\ 0 & 3 & 32/3
\end{bmatrix} =
\begin{bmatrix} -3 & -9 & 37/3 \\ 0 & -3 & -32/3 \\ 0 & 0 & -37/3
\end{bmatrix} $$

\textbf{Step 5: Find $Q$}

We have $R = H_2 A^{(1)} = H_2 H_1 A$.
So, $A = (H_2 H_1)^{-1} R = (H_1^{-1} H_2^{-1}) R$.

Since $H_1$ and $H_2$ are orthogonal and symmetric,
$$A = (H_1 H_2) R$$

Thus, $Q = H_1 H_2$.
$$ Q = \left( \frac{1}{3}
\begin{bmatrix} -1 & 2 & -2 \\ 2 & 2 & 1 \\ -2 & 1 & 2
\end{bmatrix} \right)
\begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & -1 & 0
\end{bmatrix} = \frac{1}{3}
\begin{bmatrix} -1 & 2 & -2 \\ 2 & -1 & -2 \\ -2 & -2 & -1
\end{bmatrix} $$

\textbf{Conclusion}
An orthonormal basis for $R(A)$ is formed by the columns of $Q$:
$$ \left\{ \vect{q}_1, \vect{q}_2, \vect{q}_3 \right\} = \left\{ \frac{1}{3}
\begin{bmatrix} -1 \\ 2 \\ -2
\end{bmatrix}, \frac{1}{3}
\begin{bmatrix} 2 \\ -1 \\ -2
\end{bmatrix}, \frac{1}{3}
\begin{bmatrix} -2 \\ -2 \\ -1
\end{bmatrix} \right\} $$

\subsection*{2)}

$$ A^T A = (QR)^T (QR) = R^T Q^T Q R = R^T R $$
Therefore:
$$ (A^T A)^{-1} = (R^T R)^{-1} = R^{-1} (R^T)^{-1} = R^{-1} (R^{-1})^T $$

We first need to find $R^{-1}$ from $R =
\begin{bmatrix} -3 & -9 & 37/3 \\ 0 & -3 & -32/3 \\ 0 & 0 & -37/3
\end{bmatrix}$.

We can find the inverse of this upper triangular matrix by back substitution or row reduction.
$$ \left[
\begin{array}{ccc|ccc} -3 & -9 & 37/3 & 1 & 0 & 0 \\ 0 & -3 & -32/3 & 0 & 1 & 0 \\ 0 & 0 & -37/3 & 0 & 0 & 1
\end{array} \right] $$

We can get $R^{-1} = \frac{1}{111}
\begin{bmatrix} -37 & 111 & -133 \\ 0 & -37 & 32 \\ 0 & 0 & -9
\end{bmatrix}$.

Now we compute $(A^T A)^{-1} = R^{-1} (R^{-1})^T$:
$$ (R^{-1})^T = \frac{1}{111}
\begin{bmatrix} -37 & 0 & 0 \\ 111 & -37 & 0 \\ -133 & 32 & -9
\end{bmatrix} $$
\begin{align*}
(A^T A)^{-1} &= \frac{1}{111^2}
\begin{bmatrix} -37 & 111 & -133 \\ 0 & -37 & 32 \\ 0 & 0 & -9
\end{bmatrix}
\begin{bmatrix} -37 & 0 & 0 \\ 111 & -37 & 0 \\ -133 & 32 & -9
\end{bmatrix} \\
&= \frac{1}{12321}
\begin{bmatrix} 1369 + 12321 + 17689 & -4107 - 4256 & 1197 \\ -4107 - 4256 & 1369 + 1024 & -288 \\ 1197 & -288 & 81
\end{bmatrix}
\end{align*}

Thus, the inverse of $A^T A$ is:
$$ (A^T A)^{-1} = \frac{1}{12321}
\begin{bmatrix} 31379 & -8363 & 1197 \\ -8363 & 2393 & -288 \\ 1197 & -288 & 81
\end{bmatrix} $$

\section*{Problem 4. (Givens Rotation)}

\textbf{Step 1: Zero out A(4, 1)}

We apply a rotation $G_1$ to zero out $A(4, 1) = 2$ using $A(1, 1) = 1$.

Let $a = 1$ and $b = 2$.
$r = \sqrt{a^2 + b^2} = \sqrt{5}$,
$c = a/r = 1/\sqrt{5}$,
$s = b/r = 2/\sqrt{5}$

The rotation matrix $G_1$ is:
$$ G_1 =
\begin{bmatrix} c & 0 & 0 & s \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ -s & 0 & 0 & c
\end{bmatrix} =
\begin{bmatrix} 1/\sqrt{5} & 0 & 0 & 2/\sqrt{5} \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ -2/\sqrt{5} & 0 & 0 & 1/\sqrt{5}
\end{bmatrix} $$

We compute $A_1 = G_1 A$:
$$ A_1 =
\begin{bmatrix} 1/\sqrt{5} & 0 & 0 & 2/\sqrt{5} \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ -2/\sqrt{5} & 0 & 0 & 1/\sqrt{5}
\end{bmatrix}
\begin{bmatrix} 1 & 2 & 2 \\ 0 & 1 & 0 \\ 0 & 2 & 2 \\ 2 & 3 & 5
\end{bmatrix} =
\begin{bmatrix} \sqrt{5} & 8/\sqrt{5} & 12/\sqrt{5} \\ 0 & 1 & 0 \\ 0 & 2 & 2 \\ 0 & -1/\sqrt{5} & 1/\sqrt{5}
\end{bmatrix} $$

\textbf{Step 2: Zero out A\(_1\)(3, 2)}

Next, we apply a rotation $G_2$ to zero out $A_1(3, 2) = 2$ using $A_1(2, 2) = 1$.

Let $a = 1$ and $b = 2$.

$r = \sqrt{1^2 + 2^2} = \sqrt{5}$,
$c = 1/\sqrt{5}$,
$s = 2/\sqrt{5}$

$$ G_2 =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & c & s & 0 \\ 0 & -s & c & 0 \\ 0 & 0 & 0 & 1
\end{bmatrix} =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1/\sqrt{5} & 2/\sqrt{5} & 0 \\ 0 & -2/\sqrt{5} & 1/\sqrt{5} & 0 \\ 0 & 0 & 0 & 1
\end{bmatrix} $$
We compute $A_2 = G_2 A_1$:
$$ A_2 =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1/\sqrt{5} & 2/\sqrt{5} & 0 \\ 0 & -2/\sqrt{5} & 1/\sqrt{5} & 0 \\ 0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix} \sqrt{5} & 8/\sqrt{5} & 12/\sqrt{5} \\ 0 & 1 & 0 \\ 0 & 2 & 2 \\ 0 & -1/\sqrt{5} & 1/\sqrt{5}
\end{bmatrix} =
\begin{bmatrix} \sqrt{5} & 8/\sqrt{5} & 12/\sqrt{5} \\ 0 & \sqrt{5} & 4/\sqrt{5} \\ 0 & 0 & 2/\sqrt{5} \\ 0 & -1/\sqrt{5} & 1/\sqrt{5}
\end{bmatrix} $$

\textbf{Step 3: Zero out A\(_2\)(4, 2)}

We apply a rotation $G_3$ to zero out $A_2(4, 2) = -1/\sqrt{5}$ using $A_2(2, 2) = \sqrt{5}$.

Let $a = \sqrt{5}$ and $b = -1/\sqrt{5}$.
$r = \sqrt{(\sqrt{5})^2 + (-1/\sqrt{5})^2} = \sqrt{26/5}$,
$c = a/r = \sqrt{5} / \sqrt{26/5} = 5/\sqrt{26}$,
$s = b/r = (-1/\sqrt{5}) / \sqrt{26/5} = -1/\sqrt{26}$

$$ G_3 =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & c & 0 & s \\ 0 & 0 & 1 & 0 \\ 0 & -s & 0 & c
\end{bmatrix} =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 5/\sqrt{26} & 0 & -1/\sqrt{26} \\ 0 & 0 & 1 & 0 \\ 0 & 1/\sqrt{26} & 0 & 5/\sqrt{26}
\end{bmatrix} $$
We compute $A_3 = G_3 A_2$:
$$ A_3 =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 5/\sqrt{26} & 0 & -1/\sqrt{26} \\ 0 & 0 & 1 & 0 \\ 0 & 1/\sqrt{26} & 0 & 5/\sqrt{26}
\end{bmatrix}
\begin{bmatrix} \sqrt{5} & 8/\sqrt{5} & 12/\sqrt{5} \\ 0 & \sqrt{5} & 4/\sqrt{5} \\ 0 & 0 & 2/\sqrt{5} \\ 0 & -1/\sqrt{5} & 1/\sqrt{5}
\end{bmatrix} =
\begin{bmatrix} \sqrt{5} & 8/\sqrt{5} & 12/\sqrt{5} \\ 0 & \sqrt{26/5} & 19/\sqrt{130} \\ 0 & 0 & 2/\sqrt{5} \\ 0 & 0 & 9/\sqrt{130}
\end{bmatrix} $$

\textbf{Step 4: Zero out A\(_3\)(4, 3)}

Finally, we apply a rotation $G_4$ to zero out $A_3(4, 3) = 9/\sqrt{130}$ using $A_3(3, 3) = 2/\sqrt{5}$.

Let $a = 2/\sqrt{5}$ and $b = 9/\sqrt{130}$.

$r = \sqrt{(2/\sqrt{5})^2 + (9/\sqrt{130})^2} = \sqrt{37/26}$,
$c = a/r = (2/\sqrt{5}) / \sqrt{185/130} = 2\sqrt{26}/\sqrt{185}$,
$s = b/r = (9/\sqrt{130}) / \sqrt{185/130} = 9/\sqrt{185}$

$$ G_4 =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & c & s \\ 0 & 0 & -s & c
\end{bmatrix} =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 2\sqrt{26}/\sqrt{185} & 9/\sqrt{185} \\ 0 & 0 & -9/\sqrt{185} & 2\sqrt{26}/\sqrt{185}
\end{bmatrix} $$
We compute $R = G_4 A_3$:
$$ R =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & c & s \\ 0 & 0 & -s & c
\end{bmatrix}
\begin{bmatrix} \sqrt{5} & 8/\sqrt{5} & 12/\sqrt{5} \\ 0 & \sqrt{26/5} & 19/\sqrt{130} \\ 0 & 0 & 2/\sqrt{5} \\ 0 & 0 & 9/\sqrt{130}
\end{bmatrix} =
\begin{bmatrix} \sqrt{5} & 8/\sqrt{5} & 12/\sqrt{5} \\ 0 & \sqrt{26/5} & 19/\sqrt{130} \\ 0 & 0 & \sqrt{185/130} \\ 0 & 0 & 0
\end{bmatrix} $$
This gives our upper triangular matrix $R$.

\textbf{Final Matrix R}

Rationalizing the denominators, $R$ is:
$$ R =
\begin{bmatrix}
\sqrt{5} & \frac{8\sqrt{5}}{5} & \frac{12\sqrt{5}}{5} \\
0 & \frac{\sqrt{130}}{5} & \frac{19\sqrt{130}}{130} \\
0 & 0 & \frac{\sqrt{962}}{26} \\
0 & 0 & 0
\end{bmatrix} $$

\textbf{Finding the Matrix Q}

Now we find $Q = G_1^T G_2^T G_3^T G_4^T$.
$$ G_1^T =
\begin{bmatrix} 1/\sqrt{5} & 0 & 0 & -2/\sqrt{5} \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 2/\sqrt{5} & 0 & 0 & 1/\sqrt{5}
\end{bmatrix}, \quad G_2^T =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1/\sqrt{5} & -2/\sqrt{5} & 0 \\ 0 & 2/\sqrt{5} & 1/\sqrt{5} & 0 \\ 0 & 0 & 0 & 1
\end{bmatrix} $$
$$ G_3^T =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 5/\sqrt{26} & 0 & 1/\sqrt{26} \\ 0 & 0 & 1 & 0 \\ 0 & -1/\sqrt{26} & 0 & 5/\sqrt{26}
\end{bmatrix}, \quad G_4^T =
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & \frac{2\sqrt{26}}{\sqrt{185}} & \frac{-9}{\sqrt{185}} \\ 0 & 0 & \frac{9}{\sqrt{185}} & \frac{2\sqrt{26}}{\sqrt{185}}
\end{bmatrix} $$

\textbf{Final Matrix Q}

Since $Q = G_1^T G_2^T G_3^T G_4^T$, we have:
$$ Q =
\begin{bmatrix}
1/\sqrt{5} & 2/\sqrt{130} & -18/\sqrt{962} & -4/\sqrt{37} \\
0 & 5/\sqrt{130} & -19/\sqrt{962} & 4/\sqrt{37} \\
0 & 10/\sqrt{130} & 14/\sqrt{962} & -1/\sqrt{37} \\
2/\sqrt{5} & -1/\sqrt{130} & 9/\sqrt{962} & 2/\sqrt{37}
\end{bmatrix}
$$

\section*{Problem 5. (QR decomposition)}

This inequality is known as \textbf{Hadamard's inequality}. We will prove it using the QR decomposition, as suggested by the problem context.

\begin{enumerate}
\item \textbf{QR Decomposition} \\
Let $A = QR$ be the QR decomposition of the (square) matrix $A$.
\begin{itemize}
\item $Q$ is an $n \times n$ orthogonal matrix, which means $Q^T Q = I$ and $Q^{-1} = Q^T$.
\item $R$ is an $n \times n$ upper triangular matrix.
\end{itemize}

\item \textbf{Determinant of A} \\
We begin by taking the absolute value of the determinant of $A$:
$$
|\det(A)| = |\det(QR)| = |\det(Q) \det(R)| = |\det(Q)| |\det(R)|
$$

\item \textbf{Determinant of Q} \\
Since $Q$ is orthogonal, we know $Q^T Q = I$. Taking the determinant of both sides:
$$
\det(Q^T Q) = \det(I)
$$
$$
\det(Q^T) \det(Q) = 1
$$
Since $\det(Q^T) = \det(Q)$, this gives $(\det(Q))^2 = 1$, which implies $|\det(Q)| = 1$.

\item \textbf{Simplifying the Determinant} \\
Substituting $|\det(Q)| = 1$ back into our equation from Step 2, we get:
$$
|\det(A)| = 1 \cdot |\det(R)| = |\det(R)|
$$

\item \textbf{Determinant of R} \\
Since $R$ is an upper triangular matrix, its determinant is the product of its diagonal entries. Let $r_{ii}$ be the diagonal entries of $R$.
$$
\det(R) = r_{11} r_{22} \cdots r_{nn}
$$
Therefore,
$$
|\det(A)| = |r_{11} r_{22} \cdots r_{nn}| = |r_{11}| |r_{22}| \cdots |r_{nn}|
$$

\item \textbf{Relating $R$ to $A$'s Columns} \\
From $A = QR$, we can look at the $i$-th column of $A$, which is $\mathbf{a}_i$. This column is the result of $Q$ multiplying the $i$-th column of $R$, which we will call $\mathbf{r}_i$.
$$
\mathbf{a}_i = Q \mathbf{r}_i
$$
Now, let's find the 2-norm (Euclidean length) of $\mathbf{a}_i$. Because $Q$ is an orthogonal matrix, it preserves the 2-norm (i.e., it is an isometry).
$$
\|\mathbf{a}_i\|_2^2 = \mathbf{a}_i^T \mathbf{a}_i = (Q \mathbf{r}_i)^T (Q \mathbf{r}_i) = \mathbf{r}_i^T Q^T Q \mathbf{r}_i = \mathbf{r}_i^T I \mathbf{r}_i = \mathbf{r}_i^T \mathbf{r}_i = \|\mathbf{r}_i\|_2^2
$$
Thus, $\|\mathbf{a}_i\|_2 = \|\mathbf{r}_i\|_2$.

\item \textbf{Norm of $R$'s Columns} \\
Because $R$ is upper triangular, its $i$-th column $\mathbf{r}_i$ only has non-zero entries in the first $i$ positions.
$$
\mathbf{r}_i =
\begin{pmatrix} r_{1i} \\ r_{2i} \\ \vdots \\ r_{ii} \\ 0 \\ \vdots \\ 0
\end{pmatrix}
$$
The squared norm of $\mathbf{r}_i$ is the sum of the squares of its components:
$$
\|\mathbf{r}_i\|_2^2 = |r_{1i}|^2 + |r_{2i}|^2 + \cdots + |r_{ii}|^2
$$

\item \textbf{The Key Inequality} \\
From Step 6 and Step 7, we can equate the squared norms:
$$
\|\mathbf{a}_i\|_2^2 = |r_{1i}|^2 + |r_{2i}|^2 + \cdots + |r_{ii}|^2
$$
Since all the terms in the sum are non-negative (as they are squares), this sum must be greater than or equal to any single term in it. In particular, it must be greater than or equal to the last non-zero term, $|r_{ii}|^2$.
$$
\|\mathbf{a}_i\|_2^2 \ge |r_{ii}|^2
$$
Taking the square root of both sides (and knowing norms are non-negative):
$$
\|\mathbf{a}_i\|_2 \ge |r_{ii}|
$$
This holds for all $i = 1, \ldots, n$.

\item \textbf{Final Assembly} \\
We now return to our determinant equation from Step 5 and apply the inequality from Step 8 for each term in the product.
$$
|\det(A)| = |r_{11}| |r_{22}| \cdots |r_{nn}|
$$
Since $\|\mathbf{a}_1\|_2 \ge |r_{11}|$, $\|\mathbf{a}_2\|_2 \ge |r_{22}|$, and so on, we can substitute them:
$$
|\det(A)| \le \|\mathbf{a}_1\|_2 \|\mathbf{a}_2\|_2 \cdots \|\mathbf{a}_n\|_2
$$

\end{enumerate}

This completes the proof.
\hfill $\Box$

\section*{Problem 6. (The Application of QR Decomposition)}
\subsection*{1)}

We are given $A \in \mathbb{R}^{m \times n}$ with $m \ge n$. We are also given that the $n \times n$ submatrix $A_1$ is nonsingular. This implies that $A$ has full column rank, i.e., $\text{rank}(A) = n$.

For a matrix $A$ with full column rank, the pseudoinverse (or left inverse) $A^\dagger$ is given by the formula:
$$
A^\dagger = (A^T A)^{-1} A^T
$$
We are instructed to use the Thin QR Decomposition, $A = QR$, where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is upper triangular. The property of $Q$ having orthonormal columns means that $Q^T Q = I_n$, where $I_n$ is the $n \times n$ identity matrix.

Since $A_1$ is nonsingular, $A$ has full column rank, which implies that $R$ is also nonsingular (invertible).

Now, we substitute $A = QR$ into the formula for $A^\dagger$:
\begin{align*}
A^\dagger &= \left( (QR)^T (QR) \right)^{-1} (QR)^T \\
&= \left( R^T Q^T Q R \right)^{-1} (R^T Q^T) \\
\end{align*}
Since $Q^T Q = I_n$, the expression simplifies:
\begin{align*}
A^\dagger &= \left( R^T (I_n) R \right)^{-1} (R^T Q^T) \\
&= \left( R^T R \right)^{-1} (R^T Q^T) \\
\end{align*}
Using the property $(AB)^{-1} = B^{-1} A^{-1}$ for invertible matrices $A$ and $B$:
\begin{align*}
A^\dagger &= \left( R^{-1} (R^T)^{-1} \right) (R^T Q^T) \\
\end{align*}
The $(R^T)^{-1}$ and $R^T$ terms cancel out, as $(R^T)^{-1} R^T = I_n$:
\begin{align*}
A^\dagger &= R^{-1} \left( (R^T)^{-1} R^T \right) Q^T \\
&= R^{-1} (I_n) Q^T \\
&= R^{-1} Q^T
\end{align*}

\noindent\textbf{The pseudoinverse is $A^\dagger = R^{-1} Q^T$.}

\subsection*{2)}

We will use the result from Part 1, the partitioning of $A$ and $Q$, and the properties given in the hint.

From the partitioning, we have $A =
\begin{bmatrix} A_1 \\ A_2
\end{bmatrix} =
\begin{bmatrix} Q_1 \\ Q_2
\end{bmatrix} R$.
This implies $A_1 = Q_1 R$.

Since $A_1$ is nonsingular (given) and $R$ is nonsingular (from Part 1), $Q_1$ must also be nonsingular. We can therefore write:
$$
R = Q_1^{-1} A_1
$$
Inverting this gives:
$$
R^{-1} = (Q_1^{-1} A_1)^{-1} = A_1^{-1} (Q_1^{-1})^{-1} = A_1^{-1} Q_1
$$
Now, let's start with the expression for $A^\dagger$ from Part 1 and find its spectral norm (2-norm):
$$
\|A^\dagger\|_2 = \|R^{-1} Q^T\|_2
$$
Using the submultiplicative property of matrix norms ($\|XY\|_2 \le \|X\|_2 \|Y\|_2$):
$$
\|A^\dagger\|_2 \le \|R^{-1}\|_2 \cdot \|Q^T\|_2
$$
The hint provides as property (a) that $\|Q^T\|_2 = 1$. Substituting this, we get:
\begin{equation}
\|A^\dagger\|_2 \le \|R^{-1}\|_2 \label{eq:1}
\end{equation}
Now, let's use the expression we found for $R^{-1}$ in terms of $A_1$ and $Q_1$:
$$
\|R^{-1}\|_2 = \|A_1^{-1} Q_1\|_2
$$
Again, using the submultiplicative property:
$$
\|R^{-1}\|_2 \le \|A_1^{-1}\|_2 \cdot \|Q_1\|_2
$$
The problem's hint provides that $\|Q_1\|_2 \le 1$. Substituting this, we get:
\begin{equation}
\|R^{-1}\|_2 \le \|A_1^{-1}\|_2 \cdot 1 = \|A_1^{-1}\|_2 \label{eq:2}
\end{equation}
Finally, we combine inequalities \eqref{eq:1} and \eqref{eq:2}:
$$
\|A^\dagger\|_2 \le \|R^{-1}\|_2 \quad \text{and} \quad \|R^{-1}\|_2 \le \|A_1^{-1}\|_2
$$
By transitivity, this proves the desired result:
$$
\|A^\dagger\|_2 \le \|A_1^{-1}\|_2
$$

\end{document}
